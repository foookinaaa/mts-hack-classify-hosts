{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6d1c440",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import validators\n",
    "import json\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be31e8b",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff8699cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/host.zip', names=['host']).drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ad3dc4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/good_hosts.txt') as f:\n",
    "    good_hosts = set(json.load(f))\n",
    "\n",
    "with open('data/tlds-alpha-by-domain.txt') as f:\n",
    "    domain_root_zones = f.readlines()\n",
    "    domain_root_zones = [\n",
    "        d[:-1].lower()\n",
    "        for d in domain_root_zones\n",
    "    ]\n",
    "\n",
    "with open('data/all_english_words.json') as f:\n",
    "    english_words = json.load(f)\n",
    "\n",
    "def is_contains_english_words(s):\n",
    "    s = s.lower()\n",
    "    return any(\n",
    "        w in s\n",
    "        for w in english_words\n",
    "    )\n",
    "\n",
    "\n",
    "re_is_ip = re.compile('^((25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\\.){3}(25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)$')\n",
    "re_m = re.compile('^m[0-9]+')\n",
    "re_2numbers = re.compile('\\d{3}')\n",
    "re_dynamic_prefix = re.compile('^[a-z]{1,2}[0-9]{1,}[\\-\\._]')\n",
    "re_cache_prefix = re.compile('cache-?[0-9]{1,2}')\n",
    "re_digits_only = re.compile('^[0-9]+$')\n",
    "re_infra_prefix = re.compile('^infra-[0-9]+')\n",
    "\n",
    "www_ignore_pattern = [\n",
    "    'www.gstatic.com',\n",
    "    'www.googleadservices.com',\n",
    "    'www.googleapis.com',    \n",
    "    'www.google-analytics.com',\n",
    "    'www.googletagmanager.com',\n",
    "    'www.tns-counter.ru',\n",
    "    'www.googletagservices.com',\n",
    "    'cdn'\n",
    "]\n",
    "\n",
    "tech_patterns_contains = [\n",
    "    'api.',\n",
    "    '.api',\n",
    "    'cdn',\n",
    "    'ad.',\n",
    "    'ads.',\n",
    "    'static.',\n",
    "    's3.',\n",
    "    'cache.',\n",
    "    'stat.',\n",
    "    'logs.',\n",
    "    'log.',\n",
    "    'stats.',\n",
    "    'auth.',\n",
    "    'sentry.',\n",
    "    'script.',\n",
    "    'storage.',\n",
    "    '--',\n",
    "    'an.yandex.ru',\n",
    "    'app-measurement.com',    \n",
    "    'tpc.googlesyndication.com',\n",
    "    'tpc.googlesyndication.com',\n",
    "    'favicon.yandex.net',\n",
    "    'googlesyndication.com',\n",
    "]\n",
    "tech_pattern_starts = [\n",
    "    'api',\n",
    "    'proxy',\n",
    "    'log',\n",
    "    'static',\n",
    "    'counter',\n",
    "    'sync.',\n",
    "    's.',\n",
    "    'a.',\n",
    "    'c.',\n",
    "    'pixel.',\n",
    "    'v1.',\n",
    "    'ssp.',\n",
    "    'img.',\n",
    "    'rtb.',\n",
    "    'code.',\n",
    "    'cm.',\n",
    "    't.',\n",
    "    'app.',\n",
    "    'grs.',\n",
    "    'analytics.',\n",
    "    'match.',\n",
    "    'adservice.',\n",
    "    'data.',\n",
    "    'd.',\n",
    "    'mc.',\n",
    "    'track.',\n",
    "    'assets',\n",
    "    'st.',\n",
    "    'js',\n",
    "    'connect.',\n",
    "    'media.',\n",
    "    'pagead2.',\n",
    "    'dl.',\n",
    "    'ajax.',\n",
    "    'content.',\n",
    "    'i.',\n",
    "    'tracking.',\n",
    "    'graph.',\n",
    "    'banners.',\n",
    "    'widget.',\n",
    "    'abtest.',\n",
    "    'strm.yandex.ru',\n",
    "    'yabs.yandex.ru',\n",
    "    'push.yandex.ru',\n",
    "    'bs.yandex.ru',\n",
    "    'statistics.',\n",
    "    'tags.',\n",
    "    'cs',\n",
    "    'adx',\n",
    "    'img',\n",
    "    'image',\n",
    "    'ads',\n",
    "    'ct.',\n",
    "    'pics.',\n",
    "    'clk.',\n",
    "    'notify.',\n",
    "    'data',\n",
    "    'ocsp.',\n",
    "    'files.',\n",
    "    'dl-',\n",
    "    'token.',\n",
    "    'graphql.',\n",
    "    'pushserver',\n",
    "    'balancer.',\n",
    "    'go.',\n",
    "    'informer.',\n",
    "    'clck.',\n",
    "    'clicks.',\n",
    "    'click.',\n",
    "    'target.',\n",
    "    'xray.',\n",
    "    'tiles.',\n",
    "    'gridserver.',\n",
    "    'metrika.',\n",
    "    'ntp.',\n",
    "    'fronterr.',\n",
    "    'lib.',\n",
    "    'tracker',\n",
    "    'appgateway',\n",
    "    'frontend.',\n",
    "    'mfa.',\n",
    "    'gate.',\n",
    "    'edge.',\n",
    "    'chat.',\n",
    "    'config.',\n",
    "    'amp.',\n",
    "    'widgets.',\n",
    "    'dev.',\n",
    "    'admin.',\n",
    "    'health.',\n",
    "    'callback.',\n",
    "    'post.',\n",
    "    'xxx-files',\n",
    "    'cluster.',\n",
    "    'ext.',\n",
    "    'file.',\n",
    "    'links.',\n",
    "    'metrics',\n",
    "]\n",
    "\n",
    "tech_patterns_ends = [\n",
    "    '.local',\n",
    "    'googleapis.com',\n",
    "    'googleusercontent.com',\n",
    "    'vkuser.net',\n",
    "    '.akamai',\n",
    "    '.link',\n",
    "    '.googleadservices',\n",
    "    '.googleadserv',\n",
    "]\n",
    "\n",
    "non_tech_pattern_starts = set([\n",
    "    'www.',\n",
    "    'm.',\n",
    "    \"maps\",\n",
    "  \"video\",\n",
    "  \"online\",\n",
    "  \"news\",\n",
    "  \"forum\",\n",
    "  \"berezniki\",\n",
    "  \"mobile\",\n",
    "    \"mail\",\n",
    "    'web.',\n",
    "    'pda.',\n",
    "    'wap.',\n",
    "])\n",
    "\n",
    "\n",
    "def predict_baseline(s):\n",
    "    if re_is_ip.search(s) is not None:\n",
    "        return True\n",
    "    \n",
    "    if not (validators.domain(s) is True):\n",
    "        return True\n",
    "    \n",
    "    if not any(s.endswith(p) for p in domain_root_zones):\n",
    "        return True\n",
    "    \n",
    "    if any(p in s for p in www_ignore_pattern):\n",
    "        return True\n",
    "    \n",
    "    if any(s.endswith(p) for p in tech_patterns_ends):\n",
    "        return True\n",
    "\n",
    "    if any(s.startswith(p) for p in non_tech_pattern_starts):\n",
    "        return False    \n",
    "    \n",
    "    if s in good_hosts:\n",
    "        return True\n",
    "    \n",
    "    \n",
    "    return any(p in s for p in tech_patterns_contains) or (\n",
    "        len(s.split('.')) > 3 and not s.startswith('www.') and not s.startswith('m.')\n",
    "    ) or any(s.startswith(p) for p in tech_pattern_starts) or (\n",
    "        len(s.split('.')[0]) > 10 and len(s.split('.')) >= 3\n",
    "    ) or re_m.search(s) is not None or (\n",
    "        s.endswith('google.com') and s != 'www.google.com'\n",
    "    ) or (\n",
    "        len(s.split('.')) == 2 and len(s) > 25 and re_2numbers.search(s) is not None and '-' in s\n",
    "    ) or (\n",
    "        len(s.split('.')) > 2 and 'api' in s.split('.')[0]\n",
    "    ) or (\n",
    "        len(s.split('.')) > 2 and 'node' in s.split('.')[0]\n",
    "    ) or (\n",
    "        len(s.split('.')) > 2 and s.split('.')[0].startswith('s')\n",
    "    ) or (\n",
    "        len(s.split('.')) > 2 and re_dynamic_prefix.search(s) is not None\n",
    "    ) or (\n",
    "        len(s.split('.')) > 2 and re_cache_prefix.search(s) is not None\n",
    "    ) or (\n",
    "        len(s.split('.')) > 2 and re_digits_only.search(s.split('.')[0]) is not None\n",
    "    ) or (\n",
    "        len(s.split('.')) > 2 and len(s.split('.')[0]) == 1 and s.split('.')[0] != 'm'\n",
    "    ) or (\n",
    "        len(s.split('.')) > 2 and len(s.split('.')[0]) == 2 and s.split('.')[0] not in {\n",
    "            'ru', 'en', 'de', 'us'\n",
    "        }\n",
    "    ) or (\n",
    "        len(s.split('.')) > 2 and re_infra_prefix.search(s.split('.')[0]) is not None\n",
    "    ) or (\n",
    "        len(s.split('.')) > 2 and 'auth' in s.split('.')[0]\n",
    "    ) or (\n",
    "        len(s.split('.')) > 2 and s.split('.')[0] in {\n",
    "            'us-east-2',\n",
    " 'us-east-1',\n",
    " 'us-west-1',\n",
    " 'us-west-2',\n",
    " 'af-south-1',\n",
    " 'ap-east-1',\n",
    " 'ap-south-1',\n",
    " 'ap-northeast-3',\n",
    " 'ap-northeast-2',\n",
    " 'ap-southeast-1',\n",
    " 'ap-southeast-2',\n",
    " 'ap-northeast-1',\n",
    " 'ca-central-1',\n",
    " 'eu-central-1',\n",
    " 'eu-west-1',\n",
    " 'eu-west-2',\n",
    " 'eu-south-1',\n",
    " 'eu-west-3',\n",
    " 'eu-north-1',\n",
    " 'me-south-1',\n",
    " 'sa-east-1',\n",
    " 'us-gov-east-1',\n",
    " 'us-gov-west-1'\n",
    "        }\n",
    "    ) or (\n",
    "        len(s.split('.')) > 2 and 'counter' in s.split('.')[0]\n",
    "    ) or (\n",
    "        len(s.split('.')) > 2 and not is_contains_english_words(s.split('.')[0])\n",
    "    ) or (\n",
    "        not is_contains_english_words(\n",
    "            ''.join(s.split('.')[:-1])\n",
    "        )\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65d01cd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mandreev/Documents/Personal/mts_hack_20210814/notebooks/venv/lib/python3.6/site-packages/ipykernel_launcher.py:4: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ae1aae305004eccb5637f81859d7bbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/199944 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hosts = df['host'].values.tolist()\n",
    "is_tech = [\n",
    "    predict_baseline(host)\n",
    "    for host in tqdm_notebook(hosts)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59c6f209",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['is_tech'] = is_tech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ec53440",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8432161005081423"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['is_tech'].sum() / len(df['is_tech']) # 0.8426744585074386"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f448556c",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3755e6ba",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "efaefc08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import dill\n",
    "import pandas as pd\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import eli5\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "687fce24",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = df[['host']], df['is_tech'].values.astype(int).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6860e467",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X[['host']], y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ee0e2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ngrams():\n",
    "    vectorizer = TfidfVectorizer(analyzer='char', ngram_range=(3, 3))\n",
    "    X_train_tfidf = vectorizer.fit_transform(X_train['host'].str.replace('.', ''))\n",
    "    vectorizer_model = LogisticRegression(C=5e1, solver='liblinear', random_state=17, n_jobs=1)\n",
    "    vectorizer_model.fit(X_train_tfidf, [int(y_i) for y_i in y_train])\n",
    "\n",
    "    vectorizer_model_weights = eli5.formatters.as_dataframe.explain_weights_df(\n",
    "        estimator=vectorizer_model, \n",
    "        feature_names= list(vectorizer.get_feature_names()),\n",
    "        top=(100, 10)\n",
    "    )\n",
    "    return vectorizer_model_weights.set_index('feature')['weight'].to_dict()\n",
    "\n",
    "ngrams = create_ngrams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "944ad40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features(hosts, ngram):\n",
    "    import pandas as pd\n",
    "    import re\n",
    "    import validators\n",
    "\n",
    "    df_features = pd.DataFrame()\n",
    "    df_features['host'] = hosts\n",
    "    df_features['url_len'] = df_features['host'].apply(lambda s: len(s))\n",
    "    df_features['max_domain_level'] = df_features['host'].apply(lambda s: len(s.split('.')))\n",
    "    df_features['max_domain_part_len'] = df_features['host'].apply(\n",
    "        lambda s: max((len(s_i) for s_i in s.split('.'))))\n",
    "\n",
    "    df_features['ngram_max'] = df_features['host'].apply(lambda s: max([\n",
    "        ngrams[ngram]\n",
    "        for ngram in ngrams.keys()\n",
    "        if ngram in s\n",
    "    ] + [0]))\n",
    "\n",
    "    df_features['ngram_min'] = df_features['host'].apply(lambda s: min([\n",
    "        ngrams[ngram]\n",
    "        for ngram in ngrams.keys()\n",
    "        if ngram in s\n",
    "    ] + [0]))\n",
    "\n",
    "    re_digit = re.compile('\\D')\n",
    "    df_features['digits_count'] = df_features['host'].apply(lambda s: len(re_digit.sub('', s)))\n",
    "\n",
    "    del df_features['host']\n",
    "\n",
    "    return df_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8cfc3d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = create_features(X_train['host'].tolist(), ngrams)\n",
    "X_test = create_features(X_test['host'].tolist(), ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b62e5d5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "digits_count           28.664959\n",
      "max_domain_level       22.542149\n",
      "url_len                19.697098\n",
      "ngram_max              19.439657\n",
      "ngram_min               8.054562\n",
      "max_domain_part_len     1.601575\n",
      "dtype: float64\n",
      "precision=0.95\n",
      "recall=0.97\n"
     ]
    }
   ],
   "source": [
    "model = CatBoostClassifier(random_state=0, verbose=0, n_estimators=11, max_depth=2)\n",
    "booster = model.fit(X_train, y_train)\n",
    "print(pd.Series(dict(zip(booster.feature_names_, booster.feature_importances_))).sort_values(ascending=False))\n",
    "predicts = booster.predict(X_test)\n",
    "print(f'precision={round(precision_score(y_test, predicts), 2)}')\n",
    "print(f'recall={round(recall_score(y_test, predicts), 2)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "284c0f98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.06366148, 0.94470798, 0.94470798, 0.94470798, 0.3037926 ,\n",
       "       0.3037926 ])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "golden_url = ['yandex.ru', 'api.yandex.ru', 'cdn.vtb.ru', 'no-cdn.vtb.ru', 'rbc.ru', 'ya.ru']\n",
    "\n",
    "booster.predict_proba(create_features(golden_url, ngrams))[:, 1]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d666604d",
   "metadata": {},
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "model = XGBClassifier(random_state=0, objective='binary:logistic')\n",
    "booster = model.fit(X_train, y_train)\n",
    "print(pd.Series(dict(zip(booster.feature_names_, booster.feature_importances_))).sort_values(ascending=False))\n",
    "predicts = booster.predict(X_test)\n",
    "print(f'precision={round(precision_score(y_test, predicts), 2)}')\n",
    "print(f'recall={round(recall_score(y_test, predicts), 2)}')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d076ee08",
   "metadata": {},
   "source": [
    "import shap\n",
    "\n",
    "explainer = shap.Explainer(model)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2173958d",
   "metadata": {},
   "source": [
    "golden_url = ['yandex.ru', 'api.yandex.ru', 'cdn.vtb.ru', 'no-cdn.vtb.ru', 'rbc.ru', 'ya.ru', 'c1asdasd.com']\n",
    "\n",
    "booster.predict_proba(create_features(golden_url, ngrams))[:, 1]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8d09bf27",
   "metadata": {},
   "source": [
    "import lightgbm\n",
    "\n",
    "model = lightgbm.LGBMClassifier()\n",
    "booster = model.fit(X_train, y_train)\n",
    "print(pd.Series(dict(zip(booster.feature_name_, booster.feature_importances_))).sort_values(ascending=False))\n",
    "predicts = booster.predict(X_test)\n",
    "print(f'precision={round(precision_score(y_test, predicts), 2)}')\n",
    "print(f'recall={round(recall_score(y_test, predicts), 2)}')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9cb125e2",
   "metadata": {},
   "source": [
    "golden_url = ['yandex.ru', 'api.yandex.ru', 'cdn.vtb.ru', 'no-cdn.vtb.ru', 'ya.ru']\n",
    "\n",
    "booster.predict_proba(create_features(golden_url, ngrams))[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c99f7b",
   "metadata": {},
   "source": [
    "# Export model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3c7a83c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExternalPredictor:\n",
    "    def __init__(self, booster, ngrams):\n",
    "        self._booster = booster\n",
    "        self._ngrams = ngrams\n",
    "\n",
    "    def predict(self, hosts):\n",
    "        return self._predict(self._booster, self._create_features(hosts, self._ngrams))\n",
    "\n",
    "    @classmethod\n",
    "    def _create_features(cls, hosts, ngrams):\n",
    "        import pandas as pd\n",
    "        import re\n",
    "\n",
    "        df_features = pd.DataFrame()\n",
    "        df_features['host'] = hosts\n",
    "        df_features['url_len'] = df_features['host'].apply(lambda s: len(s))\n",
    "        df_features['max_domain_level'] = df_features['host'].apply(lambda s: len(s.split('.')))\n",
    "        df_features['max_domain_part_len'] = df_features['host'].apply(\n",
    "            lambda s: max((len(s_i) for s_i in s.split('.'))))\n",
    "\n",
    "        df_features['ngram_max'] = df_features['host'].apply(lambda s: max([\n",
    "            ngrams[ngram]\n",
    "            for ngram in ngrams.keys()\n",
    "            if ngram in s\n",
    "        ] + [0]))\n",
    "\n",
    "        df_features['ngram_min'] = df_features['host'].apply(lambda s: min([\n",
    "            ngrams[ngram]\n",
    "            for ngram in ngrams.keys()\n",
    "            if ngram in s\n",
    "        ] + [0]))\n",
    "\n",
    "        re_digit = re.compile('\\D')\n",
    "        df_features['digits_count'] = df_features['host'].apply(lambda s: len(re_digit.sub('', s)))\n",
    "\n",
    "        del df_features['host']\n",
    "\n",
    "        return df_features\n",
    "\n",
    "    @classmethod\n",
    "    def _predict(cls, booster, df_features):\n",
    "        import shap\n",
    "\n",
    "        explainer = shap.Explainer(booster)\n",
    "        predicted_proba = round(booster.predict_proba(df_features)[0][1], 2)\n",
    "        shap_values = explainer(df_features)\n",
    "        shap_feature_importance = dict(zip(\n",
    "            shap_values.feature_names,\n",
    "            [\n",
    "                round(x, 2)\n",
    "                for x in shap_values.values[0].tolist()\n",
    "            ]\n",
    "        ))\n",
    "        predicted_proba = float(predicted_proba)\n",
    "\n",
    "        return {\n",
    "            'predict': predicted_proba > 0.5,\n",
    "            'predicted_proba': predicted_proba,\n",
    "            'shap_feature_importance': shap_feature_importance\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c27b728d",
   "metadata": {},
   "outputs": [],
   "source": [
    "booster_export = model.fit(create_features(X['host'].tolist(), ngrams), y)\n",
    "predictor = ExternalPredictor(booster_export, ngrams)\n",
    "with open('data/model.bin', 'wb') as f:\n",
    "    dill.dump(predictor, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "abadab8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'predict': True,\n",
       " 'predicted_proba': 0.94,\n",
       " 'shap_feature_importance': {'url_len': -0.5,\n",
       "  'max_domain_level': -0.01,\n",
       "  'max_domain_part_len': 0.16,\n",
       "  'ngram_max': 1.16,\n",
       "  'ngram_min': 0.08,\n",
       "  'digits_count': -1.05}}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.predict(['api.yandex.ru'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
