{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6d1c440",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import validators\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be31e8b",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5029697f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/train_v3.csv')  # выборка после паука: парсинг сайтов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ad3dc4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/good_hosts.txt') as f:  # kaggle dataset\n",
    "    good_hosts = set(json.load(f))\n",
    "\n",
    "good_hosts_v2 = {\n",
    "    h.replace('www.', '')\n",
    "    for h in good_hosts\n",
    "}\n",
    "\n",
    "with open('data/rambler.json') as f:  # nice host list\n",
    "    rambler = json.load(f)\n",
    "\n",
    "rambler_wo_www = {\n",
    "    h.replace('www.', '')\n",
    "    for h in rambler\n",
    "}\n",
    "rambler.extend(rambler_wo_www)\n",
    "\n",
    "with open('data/tlds-alpha-by-domain.txt') as f:  # domain endings\n",
    "    domain_root_zones = f.readlines()\n",
    "    domain_root_zones = [\n",
    "        d[:-1].lower()\n",
    "        for d in domain_root_zones\n",
    "    ]\n",
    "\n",
    "with open('data/all_english_words.json') as f:  # english words\n",
    "    english_words = json.load(f)\n",
    "\n",
    "sites = pd.read_excel('data/sites.xls')  # # nice host list\n",
    "good_hosts_v3_sites_wo_www = {\n",
    "    h.replace('www.', '')\n",
    "    for h in sites['URL'].values\n",
    "}\n",
    "good_hosts_v3 = list(sites['URL'].values)\n",
    "good_hosts_v3.extend(good_hosts_v3_sites_wo_www)\n",
    "\n",
    "\n",
    "def is_contains_english_words(s):\n",
    "    s = s.lower()\n",
    "    return any(\n",
    "        w in s\n",
    "        for w in english_words\n",
    "    )\n",
    "\n",
    "\n",
    "re_is_ip = re.compile('^((25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\\.){3}(25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)$')\n",
    "re_m = re.compile('^m[0-9]+')\n",
    "re_2numbers = re.compile('\\d{3}')\n",
    "re_dynamic_prefix = re.compile('^[a-z]{1,2}[0-9]{1,}[\\-\\._]')\n",
    "re_cache_prefix = re.compile('cache-?[0-9]{1,2}')\n",
    "re_digits_only = re.compile('^[0-9]+$')\n",
    "re_infra_prefix = re.compile('^infra-[0-9]+')\n",
    "\n",
    "www_ignore_pattern = [\n",
    "    'www.gstatic.com',\n",
    "    'www.googleadservices.com',\n",
    "    'www.googleapis.com',\n",
    "    'www.google-analytics.com',\n",
    "    'www.googletagmanager.com',\n",
    "    'www.tns-counter.ru',\n",
    "    'www.googletagservices.com',\n",
    "    'cdn'\n",
    "]\n",
    "\n",
    "tech_patterns_contains = [\n",
    "    'api.',\n",
    "    '.api',\n",
    "    'cdn',\n",
    "    'ad.',\n",
    "    'ads.',\n",
    "    'static.',\n",
    "    's3.',\n",
    "    'cache.',\n",
    "    'stat.',\n",
    "    'logs.',\n",
    "    'log.',\n",
    "    'stats.',\n",
    "    'auth.',\n",
    "    'sentry.',\n",
    "    'script.',\n",
    "    'storage.',\n",
    "    '--',\n",
    "    'an.yandex.ru',\n",
    "    'app-measurement.com',\n",
    "    'tpc.googlesyndication.com',\n",
    "    'tpc.googlesyndication.com',\n",
    "    'favicon.yandex.net',\n",
    "    'googlesyndication.com',\n",
    "]\n",
    "tech_pattern_starts = [\n",
    "    'api',\n",
    "    'proxy',\n",
    "    'log',\n",
    "    'static',\n",
    "    'counter',\n",
    "    'sync.',\n",
    "    's.',\n",
    "    'a.',\n",
    "    'c.',\n",
    "    'pixel.',\n",
    "    'v1.',\n",
    "    'ssp.',\n",
    "    'img.',\n",
    "    'rtb.',\n",
    "    'code.',\n",
    "    'cm.',\n",
    "    't.',\n",
    "    'app.',\n",
    "    'grs.',\n",
    "    'analytics.',\n",
    "    'match.',\n",
    "    'adservice.',\n",
    "    'data.',\n",
    "    'd.',\n",
    "    'mc.',\n",
    "    'track.',\n",
    "    'assets',\n",
    "    'st.',\n",
    "    'js',\n",
    "    'connect.',\n",
    "    'media.',\n",
    "    'pagead2.',\n",
    "    'dl.',\n",
    "    'ajax.',\n",
    "    'content.',\n",
    "    'i.',\n",
    "    'tracking.',\n",
    "    'graph.',\n",
    "    'banners.',\n",
    "    'widget.',\n",
    "    'abtest.',\n",
    "    'strm.yandex.ru',\n",
    "    'yabs.yandex.ru',\n",
    "    'push.yandex.ru',\n",
    "    'bs.yandex.ru',\n",
    "    'statistics.',\n",
    "    'tags.',\n",
    "    'cs',\n",
    "    'adx',\n",
    "    'img',\n",
    "    'image',\n",
    "    'ads',\n",
    "    'ct.',\n",
    "    'pics.',\n",
    "    'clk.',\n",
    "    'notify.',\n",
    "    'data',\n",
    "    'ocsp.',\n",
    "    'files.',\n",
    "    'dl-',\n",
    "    'token.',\n",
    "    'graphql.',\n",
    "    'pushserver',\n",
    "    'balancer.',\n",
    "    'go.',\n",
    "    'informer.',\n",
    "    'clck.',\n",
    "    'clicks.',\n",
    "    'click.',\n",
    "    'target.',\n",
    "    'xray.',\n",
    "    'tiles.',\n",
    "    'gridserver.',\n",
    "    'metrika.',\n",
    "    'ntp.',\n",
    "    'fronterr.',\n",
    "    'lib.',\n",
    "    'tracker',\n",
    "    'appgateway',\n",
    "    'frontend.',\n",
    "    'mfa.',\n",
    "    'gate.',\n",
    "    'edge.',\n",
    "    'chat.',\n",
    "    'config.',\n",
    "    'amp.',\n",
    "    'widgets.',\n",
    "    'dev.',\n",
    "    'admin.',\n",
    "    'health.',\n",
    "    'callback.',\n",
    "    'post.',\n",
    "    'xxx-files',\n",
    "    'cluster.',\n",
    "    'ext.',\n",
    "    'file.',\n",
    "    'links.',\n",
    "    'metrics',\n",
    "]\n",
    "\n",
    "tech_patterns_ends = [\n",
    "    '.local',\n",
    "    'googleapis.com',\n",
    "    'googleusercontent.com',\n",
    "    'vkuser.net',\n",
    "    '.akamai',\n",
    "    '.link',\n",
    "    '.googleadservices',\n",
    "    '.googleadserv',\n",
    "]\n",
    "\n",
    "non_tech_pattern_starts = set([\n",
    "    'www.',\n",
    "    'm.',\n",
    "    \"maps\",\n",
    "    \"video\",\n",
    "    \"online\",\n",
    "    \"news\",\n",
    "    \"forum\",\n",
    "    \"berezniki\",\n",
    "    \"mobile\",\n",
    "    \"mail\",\n",
    "    'web.',\n",
    "    'pda.',\n",
    "    'wap.',\n",
    "])\n",
    "\n",
    "\n",
    "def predict_baseline(s):\n",
    "    if re_is_ip.search(s) is not None:\n",
    "        return True\n",
    "\n",
    "    if not (validators.domain(s) is True):\n",
    "        return True\n",
    "\n",
    "    if not any(s.endswith(p) for p in domain_root_zones):\n",
    "        return True\n",
    "\n",
    "    if any(p in s for p in www_ignore_pattern):\n",
    "        return True\n",
    "\n",
    "    if any(s.endswith(p) for p in tech_patterns_ends):\n",
    "        return True\n",
    "\n",
    "    if any(s.startswith(p) for p in non_tech_pattern_starts):\n",
    "        return False\n",
    "\n",
    "    if s in good_hosts:\n",
    "        return False\n",
    "\n",
    "    if s in good_hosts_v2:  ###\n",
    "        return False\n",
    "\n",
    "    if s in good_hosts_v3:  ###\n",
    "        return False\n",
    "\n",
    "    if s in rambler:  ###\n",
    "        return False\n",
    "\n",
    "    return any(p in s for p in tech_patterns_contains) or (\n",
    "            len(s.split('.')) > 3 and not s.startswith('www.') and not s.startswith('m.')\n",
    "    ) or any(s.startswith(p) for p in tech_pattern_starts) or (\n",
    "                   len(s.split('.')[0]) > 10 and len(s.split('.')) >= 3\n",
    "           ) or re_m.search(s) is not None or (\n",
    "                   s.endswith('google.com') and s != 'www.google.com'\n",
    "           ) or (\n",
    "                   len(s.split('.')) == 2 and len(s) > 25 and re_2numbers.search(s) is not None and '-' in s\n",
    "           ) or (\n",
    "                   len(s.split('.')) > 2 and 'api' in s.split('.')[0]\n",
    "           ) or (\n",
    "                   len(s.split('.')) > 2 and 'node' in s.split('.')[0]\n",
    "           ) or (\n",
    "                   len(s.split('.')) > 2 and s.split('.')[0].startswith('s')\n",
    "           ) or (\n",
    "                   len(s.split('.')) > 2 and re_dynamic_prefix.search(s) is not None\n",
    "           ) or (\n",
    "                   len(s.split('.')) > 2 and re_cache_prefix.search(s) is not None\n",
    "           ) or (\n",
    "                   len(s.split('.')) > 2 and re_digits_only.search(s.split('.')[0]) is not None\n",
    "           ) or (\n",
    "                   len(s.split('.')) > 2 and len(s.split('.')[0]) == 1 and s.split('.')[0] != 'm'\n",
    "           ) or (\n",
    "                   len(s.split('.')) > 2 and len(s.split('.')[0]) == 2 and s.split('.')[0] not in {\n",
    "               'ru', 'en', 'de', 'us'\n",
    "           }\n",
    "           ) or (\n",
    "                   len(s.split('.')) > 2 and re_infra_prefix.search(s.split('.')[0]) is not None\n",
    "           ) or (\n",
    "                   len(s.split('.')) > 2 and 'auth' in s.split('.')[0]\n",
    "           ) or (\n",
    "                   len(s.split('.')) > 2 and s.split('.')[0] in {\n",
    "               'us-east-2',\n",
    "               'us-east-1',\n",
    "               'us-west-1',\n",
    "               'us-west-2',\n",
    "               'af-south-1',\n",
    "               'ap-east-1',\n",
    "               'ap-south-1',\n",
    "               'ap-northeast-3',\n",
    "               'ap-northeast-2',\n",
    "               'ap-southeast-1',\n",
    "               'ap-southeast-2',\n",
    "               'ap-northeast-1',\n",
    "               'ca-central-1',\n",
    "               'eu-central-1',\n",
    "               'eu-west-1',\n",
    "               'eu-west-2',\n",
    "               'eu-south-1',\n",
    "               'eu-west-3',\n",
    "               'eu-north-1',\n",
    "               'me-south-1',\n",
    "               'sa-east-1',\n",
    "               'us-gov-east-1',\n",
    "               'us-gov-west-1'\n",
    "           }\n",
    "           ) or (\n",
    "                   len(s.split('.')) > 2 and 'counter' in s.split('.')[0]\n",
    "           ) or (\n",
    "                   len(s.split('.')) > 2 and not is_contains_english_words(s.split('.')[0])\n",
    "           ) or (\n",
    "               not is_contains_english_words(\n",
    "                   ''.join(s.split('.')[:-1])\n",
    "               )\n",
    "           )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65d01cd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_26029/2164180484.py:4: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for host in tqdm_notebook(hosts)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37e8c865aaa647fa98ccf754c0a15160",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/199944 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hosts = df['host'].values.tolist()\n",
    "is_tech = [\n",
    "    predict_baseline(host)\n",
    "    for host in tqdm_notebook(hosts)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59c6f209",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['is_tech'] = is_tech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ec53440",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8425859240587364"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['is_tech'].sum() / len(df['is_tech'])  # 84% is tech"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f448556c",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3755e6ba",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "efaefc08",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import dill\n",
    "import pandas as pd\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import eli5\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "687fce24",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = df[['host']], df['is_tech_v3'].values.astype(int).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b43c8c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pd.Series(y).map({\n",
    "                         1: 0,\n",
    "                         0: 1\n",
    "                     })  # 1 - users, 0 - tech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6860e467",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X[['host']], y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ee0e2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ngrams():\n",
    "    vectorizer = TfidfVectorizer(analyzer='char', ngram_range=(3, 3))\n",
    "    X_train_tfidf = vectorizer.fit_transform(X_train['host'].str.replace('.', ''))\n",
    "    vectorizer_model = LogisticRegression(C=5e1, solver='liblinear', random_state=17, n_jobs=1)\n",
    "    vectorizer_model.fit(X_train_tfidf, [int(y_i) for y_i in y_train])\n",
    "\n",
    "    vectorizer_model_weights = eli5.formatters.as_dataframe.explain_weights_df(\n",
    "        estimator=vectorizer_model,\n",
    "        feature_names=list(vectorizer.get_feature_names()),\n",
    "        top=(100, 10)\n",
    "    )\n",
    "    return vectorizer_model_weights.set_index('feature')['weight'].to_dict()\n",
    "\n",
    "\n",
    "ngrams = create_ngrams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "944ad40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features(hosts, ngram):\n",
    "    import pandas as pd\n",
    "    import re\n",
    "\n",
    "    df_features = pd.DataFrame()\n",
    "    df_features['host'] = hosts\n",
    "    df_features['url_len'] = df_features['host'].apply(lambda s: len(s))\n",
    "    df_features['max_domain_level'] = df_features['host'].apply(lambda s: len(s.split('.')))\n",
    "    df_features['max_domain_part_len'] = df_features['host'].apply(\n",
    "        lambda s: max((len(s_i) for s_i in s.split('.'))))\n",
    "\n",
    "    df_features['ngram_max'] = df_features['host'].apply(lambda s: max([\n",
    "                                                                           ngrams[ngram]\n",
    "                                                                           for ngram in ngrams.keys()\n",
    "                                                                           if ngram in s\n",
    "                                                                       ] + [0]))\n",
    "\n",
    "    df_features['ngram_min'] = df_features['host'].apply(lambda s: min([\n",
    "                                                                           ngrams[ngram]\n",
    "                                                                           for ngram in ngrams.keys()\n",
    "                                                                           if ngram in s\n",
    "                                                                       ] + [0]))\n",
    "\n",
    "    re_digit = re.compile('\\D')\n",
    "    df_features['digits_count'] = df_features['host'].apply(lambda s: len(re_digit.sub('', s)))\n",
    "\n",
    "    del df_features['host']\n",
    "\n",
    "    return df_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8cfc3d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = create_features(X_train['host'].tolist(), ngrams)\n",
    "X_test = create_features(X_test['host'].tolist(), ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b62e5d5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_domain_level       76.847973\n",
      "url_len                14.649828\n",
      "digits_count            7.181749\n",
      "max_domain_part_len     1.320449\n",
      "ngram_min               0.000000\n",
      "ngram_max               0.000000\n",
      "dtype: float64\n",
      "precision=0.92\n",
      "recall=0.98\n"
     ]
    }
   ],
   "source": [
    "model = CatBoostClassifier(random_state=0, verbose=0, max_depth=2, n_estimators=5)\n",
    "booster = model.fit(X_train, y_train)\n",
    "print(pd.Series(dict(zip(booster.feature_names_, booster.feature_importances_))).sort_values(ascending=False))\n",
    "predicts = booster.predict(X_test)\n",
    "print(f'precision={round(precision_score(y_test, predicts), 2)}')\n",
    "print(f'recall={round(recall_score(y_test, predicts), 2)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "284c0f98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.9013329 , 0.03545867, 0.03595288, 0.03545867, 0.70261085,\n",
       "       0.70261085])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "golden_url = ['yandex.ru', 'api.yandex.ru', 'cdn.vtb.ru', 'no-cdn.vtb.ru', 'rbc.ru', 'ya.ru']\n",
    "\n",
    "booster.predict_proba(create_features(golden_url, ngrams))[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847974d7",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "863cbc65",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val_1 = pd.read_csv('validation_manual/df_non_tech_manual.csv')\n",
    "df_val_1 = df_val_1[['host', 'is_tech_manual']]\n",
    "df_val_1.loc[df_val_1['is_tech_manual'] == ' ', 'is_tech_manual'] = 1\n",
    "df_val_1.dropna(inplace=True)\n",
    "df_val_1['is_tech_manual'] = df_val_1['is_tech_manual'].astype('int')\n",
    "\n",
    "df_val_2 = pd.read_csv('validation_manual/output_AM.csv')\n",
    "df_val_2 = df_val_2[['host', 'is_tech_manual']]\n",
    "df_val_2['is_tech_manual'] = df_val_2['is_tech_manual'].astype('int')\n",
    "\n",
    "df_val = pd.concat([df_val_1, df_val_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f5a21159",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = df_val['host']\n",
    "y_val = df_val['is_tech_manual'].map({\n",
    "                                         1: 0,\n",
    "                                         0: 1\n",
    "                                     })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c9cc01dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = create_features(X_val.tolist(), ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "04f3074c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision=0.72\n",
      "recall=0.39\n"
     ]
    }
   ],
   "source": [
    "predicts = booster.predict(X_val)\n",
    "print(f'precision={round(precision_score(y_val, predicts), 2)}')\n",
    "print(f'recall={round(recall_score(y_val, predicts), 2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c99f7b",
   "metadata": {},
   "source": [
    "# Export model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3c7a83c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SharedPredictor:\n",
    "    def __init__(self, booster, ngrams):\n",
    "        self._booster = booster\n",
    "        self._ngrams = ngrams\n",
    "\n",
    "    def predict(self, hosts):\n",
    "        return self._predict(self._booster, self._create_features(hosts, self._ngrams))\n",
    "\n",
    "    @classmethod\n",
    "    def _create_features(cls, hosts, ngrams):\n",
    "        # copy features extract pipeline\n",
    "        import pandas as pd\n",
    "        import re\n",
    "\n",
    "        df_features = pd.DataFrame()\n",
    "        df_features['host'] = hosts\n",
    "        df_features['url_len'] = df_features['host'].apply(lambda s: len(s))\n",
    "        df_features['max_domain_level'] = df_features['host'].apply(lambda s: len(s.split('.')))\n",
    "        df_features['max_domain_part_len'] = df_features['host'].apply(\n",
    "            lambda s: max((len(s_i) for s_i in s.split('.'))))\n",
    "\n",
    "        df_features['ngram_max'] = df_features['host'].apply(lambda s: max([\n",
    "                                                                               ngrams[ngram]\n",
    "                                                                               for ngram in ngrams.keys()\n",
    "                                                                               if ngram in s\n",
    "                                                                           ] + [0]))\n",
    "\n",
    "        df_features['ngram_min'] = df_features['host'].apply(lambda s: min([\n",
    "                                                                               ngrams[ngram]\n",
    "                                                                               for ngram in ngrams.keys()\n",
    "                                                                               if ngram in s\n",
    "                                                                           ] + [0]))\n",
    "\n",
    "        re_digit = re.compile('\\D')\n",
    "        df_features['digits_count'] = df_features['host'].apply(lambda s: len(re_digit.sub('', s)))\n",
    "\n",
    "        del df_features['host']\n",
    "\n",
    "        return df_features\n",
    "\n",
    "    @classmethod\n",
    "    def _predict(cls, booster, df_features):\n",
    "        import shap\n",
    "\n",
    "        russian_names = {\n",
    "            'url_len': 'Длина хоста',\n",
    "            'max_domain_level': 'Количество поддоменов',\n",
    "            'max_domain_part_len': 'Максимальная длина поддомена',\n",
    "            'ngram_max': 'Максимальный вес 3х буквенной n-граммы',\n",
    "            'ngram_min': 'Минимальный вес 3х буквенной n-граммы',\n",
    "            'digits_count': 'Число цифр в хосте'\n",
    "        }\n",
    "\n",
    "        explainer = shap.Explainer(booster)\n",
    "        predicted_proba = round(booster.predict_proba(df_features)[0][1], 2)\n",
    "        shap_values = explainer(df_features)\n",
    "        shap_feature_importance = dict(zip(\n",
    "            [russian_names.get(n, n) for n in shap_values.feature_names],\n",
    "            [\n",
    "                round(x, 2)\n",
    "                for x in shap_values.values[0].tolist()\n",
    "            ]\n",
    "        ))\n",
    "        predicted_proba = float(predicted_proba)\n",
    "\n",
    "        return {\n",
    "            'predict': predicted_proba > 0.5,\n",
    "            'predicted_proba': predicted_proba,\n",
    "            'shap_feature_importance': shap_feature_importance\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c27b728d",
   "metadata": {},
   "outputs": [],
   "source": [
    "booster_export = model.fit(create_features(X['host'].tolist(), ngrams), y)\n",
    "predictor = SharedPredictor(booster_export, ngrams)\n",
    "with open('data/model.bin', 'wb') as f:\n",
    "    dill.dump(predictor, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "abadab8f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'predict': False,\n",
       " 'predicted_proba': 0.04,\n",
       " 'shap_feature_importance': {'Длина хоста': 0.07,\n",
       "  'Количество поддоменов': -0.59,\n",
       "  'Максимальная длина поддомена': 0.0,\n",
       "  'Максимальный вес 3х буквенной n-граммы': 0.0,\n",
       "  'Минимальный вес 3х буквенной n-граммы': -0.05,\n",
       "  'Число цифр в хосте': 0.12}}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.predict(['api.yandex.ru'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7824d683",
   "metadata": {},
   "source": [
    "# P.S. \n",
    "дальше, что не успели :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9fd167d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_ngrams_5():\n",
    "#     vectorizer = TfidfVectorizer(analyzer='char', ngram_range=(4, 5))\n",
    "#     X_train_tfidf = vectorizer.fit_transform(X_train['host'].str.replace('.', ''))\n",
    "#     vectorizer_model = LogisticRegression(C=5e1, solver='liblinear', random_state=17, n_jobs=1)\n",
    "#     vectorizer_model.fit(X_train_tfidf, [int(y_i) for y_i in y_train])\n",
    "\n",
    "#     vectorizer_model_weights = eli5.formatters.as_dataframe.explain_weights_df(\n",
    "#         estimator=vectorizer_model, \n",
    "#         feature_names= list(vectorizer.get_feature_names()),\n",
    "#         top=(100, 10)\n",
    "#     )\n",
    "#     return vectorizer_model_weights.set_index('feature')['weight'].to_dict()\n",
    "\n",
    "# ngrams_5 = create_ngrams_5()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "6d84e8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_features(hosts, ngram):\n",
    "#     import pandas as pd\n",
    "#     import re\n",
    "#     import validators\n",
    "\n",
    "#     df_features = pd.DataFrame()\n",
    "#     df_features['host'] = hosts\n",
    "#     df_features['url_len'] = df_features['host'].apply(lambda s: len(s))\n",
    "#     df_features['max_domain_level'] = df_features['host'].apply(lambda s: len(s.split('.')))\n",
    "#     df_features['max_domain_part_len'] = df_features['host'].apply(\n",
    "#         lambda s: max((len(s_i) for s_i in s.split('.'))))\n",
    "\n",
    "#     df_features['ngram_max'] = df_features['host'].apply(lambda s: max([\n",
    "#         ngrams[ngram]\n",
    "#         for ngram in ngrams.keys()\n",
    "#         if ngram in s\n",
    "#     ] + [0]))\n",
    "\n",
    "#     df_features['ngram_max_5'] = df_features['host'].apply(lambda s: max([\n",
    "#         ngrams_5[ngram]\n",
    "#         for ngram in ngrams_5.keys()\n",
    "#         if ngram in s\n",
    "#     ] + [0]))\n",
    "\n",
    "#     df_features['ngram_min'] = df_features['host'].apply(lambda s: min([\n",
    "#         ngrams[ngram]\n",
    "#         for ngram in ngrams.keys()\n",
    "#         if ngram in s\n",
    "#     ] + [0]))\n",
    "\n",
    "#     df_features['ngram_min_5'] = df_features['host'].apply(lambda s: min([\n",
    "#         ngrams_5[ngram]\n",
    "#         for ngram in ngrams_5.keys()\n",
    "#         if ngram in s\n",
    "#     ] + [0]))\n",
    "\n",
    "#     re_digit = re.compile('\\D')\n",
    "#     df_features['digits_count'] = df_features['host'].apply(lambda s: len(re_digit.sub('', s)))\n",
    "\n",
    "#     del df_features['host']\n",
    "\n",
    "#     return df_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "b5215977",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check = pd.concat([pd.DataFrame(y_test), pd.DataFrame(predicts)], axis=1)\n",
    "# check.columns = ['test', 'pred']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "367c0444",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check = pd.concat([X_test.reset_index(drop=True), check], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "654cb876",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# check[check['test'] != check['pred']]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}